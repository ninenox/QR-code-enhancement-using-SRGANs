{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SRGAN demonstration for QR code enhancement (PyTorch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installing dependencies + Unzipping data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!unzip archive.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install torch torchvision opencv-python tqdm scikit-image\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from main_pytorch import Generator, Discriminator, build_vgg\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Looking at a single example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "datadir = 'qr_dataset'\n",
        "\n",
        "first_image = None\n",
        "for img in os.listdir(datadir):\n",
        "    img_array = cv2.imread(os.path.join(datadir, img), cv2.IMREAD_COLOR)\n",
        "    if img_array is None:\n",
        "        continue\n",
        "    first_image = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
        "    break\n",
        "\n",
        "if first_image is not None:\n",
        "    plt.imshow(first_image)\n",
        "    plt.title('Sample QR image (RGB)')\n",
        "    plt.axis('off')\n",
        "else:\n",
        "    print('No images found in dataset.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Storing the images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "high_res_images = []\n",
        "low_res_images = []\n",
        "\n",
        "def create_training_data():\n",
        "    for img in tqdm(list(os.listdir(datadir))):\n",
        "        img_path = os.path.join(datadir, img)\n",
        "        img_array = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        if img_array is None:\n",
        "            continue\n",
        "        img_rgb = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
        "        high_res = cv2.resize(img_rgb, (128, 128))\n",
        "        low_res = cv2.resize(img_rgb, (32, 32), interpolation=cv2.INTER_AREA)\n",
        "        high_res_images.append(high_res)\n",
        "        low_res_images.append(low_res)\n",
        "\n",
        "create_training_data()\n",
        "print(f'Loaded {len(high_res_images)} images')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "high_res = np.array(high_res_images, dtype=np.float32) / 255.0\n",
        "low_res = np.array(low_res_images, dtype=np.float32) / 255.0\n",
        "print('High-res shape:', high_res.shape)\n",
        "print('Low-res shape:', low_res.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(low_res, high_res, test_size=0.2, random_state=42)\n",
        "print('Training set:', X_train.shape, y_train.shape)\n",
        "print('Validation set:', X_valid.shape, y_valid.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def numpy_to_tensor(arr):\n",
        "    tensor = torch.from_numpy(arr).permute(0, 3, 1, 2).float()\n",
        "    return tensor\n",
        "\n",
        "X_train_t = numpy_to_tensor(X_train)\n",
        "y_train_t = numpy_to_tensor(y_train)\n",
        "X_valid_t = numpy_to_tensor(X_valid)\n",
        "y_valid_t = numpy_to_tensor(y_valid)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
        "valid_dataset = TensorDataset(X_valid_t, y_valid_t)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "print('Training batches:', len(train_loader))\n",
        "print('Validation batches:', len(valid_loader))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the PyTorch models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "generator = Generator(res_blocks=1, upsample_blocks=2).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "vgg = build_vgg().to(device)\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "adversarial_criterion = nn.BCELoss()\n",
        "pixel_criterion = nn.MSELoss()\n",
        "content_criterion = nn.MSELoss()\n",
        "\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
        "\n",
        "print('Models ready on device:', device)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "imagenet_mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)\n",
        "imagenet_std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)\n",
        "\n",
        "def preprocess_vgg(x):\n",
        "    return (x - imagenet_mean) / imagenet_std\n",
        "\n",
        "def train_one_epoch(epoch):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    running_g_loss = 0.0\n",
        "    running_d_loss = 0.0\n",
        "    for lr_imgs, hr_imgs in train_loader:\n",
        "        lr_imgs = lr_imgs.to(device)\n",
        "        hr_imgs = hr_imgs.to(device)\n",
        "\n",
        "        valid = torch.ones((lr_imgs.size(0), 1), device=device)\n",
        "        fake = torch.zeros((lr_imgs.size(0), 1), device=device)\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        gen_imgs = generator(lr_imgs)\n",
        "        pred_fake = discriminator(gen_imgs).view(-1, 1)\n",
        "        g_adv = adversarial_criterion(pred_fake, valid)\n",
        "        gen_features = vgg(preprocess_vgg(gen_imgs))\n",
        "        with torch.no_grad():\n",
        "            real_features = vgg(preprocess_vgg(hr_imgs))\n",
        "        g_content = content_criterion(gen_features, real_features)\n",
        "        g_pixel = pixel_criterion(gen_imgs, hr_imgs)\n",
        "        g_loss = g_pixel + 1e-3 * g_adv + 0.006 * g_content\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        pred_real = discriminator(hr_imgs).view(-1, 1)\n",
        "        loss_real = adversarial_criterion(pred_real, valid)\n",
        "        pred_fake = discriminator(gen_imgs.detach()).view(-1, 1)\n",
        "        loss_fake = adversarial_criterion(pred_fake, fake)\n",
        "        d_loss = 0.5 * (loss_real + loss_fake)\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        running_g_loss += g_loss.item() * lr_imgs.size(0)\n",
        "        running_d_loss += d_loss.item() * lr_imgs.size(0)\n",
        "\n",
        "    epoch_g_loss = running_g_loss / len(train_dataset)\n",
        "    epoch_d_loss = running_d_loss / len(train_dataset)\n",
        "    return epoch_g_loss, epoch_d_loss\n",
        "\n",
        "def validate():\n",
        "    generator.eval()\n",
        "    pixel_losses = []\n",
        "    with torch.no_grad():\n",
        "        for lr_imgs, hr_imgs in valid_loader:\n",
        "            lr_imgs = lr_imgs.to(device)\n",
        "            hr_imgs = hr_imgs.to(device)\n",
        "            gen_imgs = generator(lr_imgs)\n",
        "            pixel_losses.append(pixel_criterion(gen_imgs, hr_imgs).item())\n",
        "    return float(np.mean(pixel_losses)) if pixel_losses else 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "train_history = []\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    g_loss, d_loss = train_one_epoch(epoch)\n",
        "    val_loss = validate()\n",
        "    train_history.append((epoch, g_loss, d_loss, val_loss))\n",
        "    print(f'Epoch {epoch}/{num_epochs} | G_loss: {g_loss:.4f} | D_loss: {d_loss:.4f} | Val pixel loss: {val_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Checking the generator output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "generator.eval()\n",
        "with torch.no_grad():\n",
        "    sample_lr = X_valid_t[:1].to(device)\n",
        "    sample_hr = y_valid_t[:1].to(device)\n",
        "    sr_image = generator(sample_lr).cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
        "    lr_image = sample_lr.cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
        "    hr_image = sample_hr.cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
        "\n",
        "sr_image = np.clip(sr_image, 0, 1)\n",
        "lr_image = np.clip(lr_image, 0, 1)\n",
        "hr_image = np.clip(hr_image, 0, 1)\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title('LR Image (Upsampled)')\n",
        "plt.imshow(cv2.resize(lr_image, (128, 128)))\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title('Superresolution (Generator)')\n",
        "plt.imshow(sr_image)\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title('Original HR Image')\n",
        "plt.imshow(hr_image)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}